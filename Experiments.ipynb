{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Jumping Knowledge Networks on Citeseer and Cora\n",
    "Here I try to replicate the evaluation of JK Networks described in the [Xu et al.](https://arxiv.org/abs/1806.03536). First Xu and colleagues test GCNs and GATs on the Citeseer and Cora datasets. They also test adding the Jumping Knowledge Aggregation to the GCN with LSTM, Max pooling and Concatenation aggregation methods. I will also test a simple MLP as a baseline. Xu and colleagues vary the number of layers from 1-6 (using a hidden layer size of 16 or 32) and choose the best performing model on the validation set then compare each of the best models on the test set. When testing I will use 3 different splits to report the mean and standard deviaiton of test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jk_networks import utils, models\n",
    "import torch\n",
    "from itertools import product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the CiteSeer dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "citeseer = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "I will now train an MLP on the citeseer dataset. This should perform worse than the GCN model because it doesn't have any graph level information but it should provide a good baseline for how well a model can do with just the bag of word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_accuracies(data):\n",
    "  num_layers = range(1, 7)\n",
    "  hidden_layer_size = [16, 32]\n",
    "  val_accuracies = defaultdict(dict)\n",
    "  for num_layers, hidden_layer_size in product(num_layers, hidden_layer_size):\n",
    "    mlp_model = models.MLP(data.num_features, [hidden_layer_size] * num_layers, data.num_classes)\n",
    "    graph = data[0]\n",
    "    utils.split_data_node_classification(graph, train_ratio=0.6, val_ratio=0.2, manual_seed=42)\n",
    "    utils.train(mlp_model, graph)\n",
    "    model_acc = utils.test(mlp_model, graph, graph.val_mask)\n",
    "    val_accuracies[num_layers][hidden_layer_size] = model_acc\n",
    "  return val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute 'lin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_mlp_accuracies(citeseer)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mget_mlp_accuracies\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m graph \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m utils\u001b[39m.\u001b[39msplit_data_node_classification(graph, train_ratio\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, val_ratio\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, manual_seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m utils\u001b[39m.\u001b[39mtrain(mlp_model, graph)\n\u001b[1;32m     10\u001b[0m model_acc \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mtest(mlp_model, graph, graph\u001b[39m.\u001b[39mval_mask)\n\u001b[1;32m     11\u001b[0m val_accuracies[num_layers][hidden_layer_size] \u001b[39m=\u001b[39m model_acc\n",
      "File \u001b[0;32m~/Documents/Programming/machinelearningprojects/geometric_deeplearning/implementing_jumping_knowledge_networks/jk_networks/utils.py:50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, graph, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     49\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m   out \u001b[39m=\u001b[39m model(graph\u001b[39m.\u001b[39mx, graph\u001b[39m.\u001b[39medge_index)\n\u001b[1;32m     51\u001b[0m   loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(out[graph\u001b[39m.\u001b[39mtrain_mask], graph\u001b[39m.\u001b[39my[graph\u001b[39m.\u001b[39mtrain_mask])\n\u001b[1;32m     52\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/machinelearningprojects/geometric_deeplearning/implementing_jumping_knowledge_networks/jk_networks/models.py:86\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     84\u001b[0m   x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m     85\u001b[0m   x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'lin'"
     ]
    }
   ],
   "source": [
    "get_mlp_accuracies(citeseer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing GCN\n",
    "Following [Xu et al.](https://arxiv.org/abs/1806.03536) I will train a series of GCNs without any Jumping Knowledge on the Citeseer dataset. I will test 12 different models with the number of layers going from 1 to 6 and the number of hidden_feauters in {16, 32}. Note Xu and collegues use a 60, 20, 20 split which is different from the built in split of citesseer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcn_accuracies(data):\n",
    "  num_layers = range(1, 7)\n",
    "  hidden_layer_size = [16, 32]\n",
    "  val_accuracies = defaultdict(dict)\n",
    "  for num_layers, hidden_layer_size in product(num_layers, hidden_layer_size):\n",
    "    gcn_model = models.GCN(data.num_features, [hidden_layer_size] * num_layers, data.num_classes)\n",
    "    graph = data[0]\n",
    "    utils.split_data_node_classification(graph, train_ratio=0.6, val_ratio=0.2, manual_seed=42)\n",
    "    utils.train(gcn_model, graph)\n",
    "    model_acc = utils.test(gcn_model, graph, graph.val_mask)\n",
    "    val_accuracies[num_layers][hidden_layer_size] = model_acc\n",
    "  return val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_accuracies = get_gcn_accuracies(citeseer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Number of Layers</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layer Size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.668</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Number of Layers       1      2      3      4      5      6\n",
       "Hidden Layer Size                                          \n",
       "32                 0.714  0.565  0.585  0.517  0.226  0.217\n",
       "16                 0.668  0.582  0.606  0.356  0.197  0.206"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies = pd.DataFrame(val_accuracies)\n",
    "val_accuracies.index.name = 'Hidden Layer Size'\n",
    "val_accuracies.columns.name = 'Number of Layers'\n",
    "val_accuracies.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "We can see that the 1 Layer, 32 hidden features network performs the best. I will retrain that model on the validation and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gcn_model = models.GCN(citeseer.num_features, [32], citeseer.num_classes)\n",
    "def train_best_model(model, data):\n",
    "  graph = data[0]\n",
    "  graph.train_mask = torch.logical_or(graph.train_mask, graph.val_mask) # train on train+val\n",
    "  utils.train(model, graph)\n",
    "  return utils.test(model, graph, graph.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gcn_model_acc = train_best_model(best_gcn_model, citeseer)\n",
    "best_gcn_model_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Continued\n",
    "As we can seee the normal GCN network is only at about 70% accuracy on the citeseer dataset. This number is different from the paper which found a GCN accuracy of 77.3% for the GCN on the citeseer dataset this could be due to some of the [preprocessing](https://github.com/pyg-team/pytorch_geometric/issues/2018) that torch_geometric does to citeseer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
